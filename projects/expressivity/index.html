<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sequence classification expressivity | Harry Langford </title> <meta name="author" content="Harry Langford"> <meta name="description" content="Comparing expressivity to learnability"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?adc86d4da45e3a559b5af9204f14c663"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hjel2.github.io/projects/expressivity/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Sequence classification expressivity",
            "description": "Comparing expressivity to learnability",
            "published": "14 February 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Harry Langford </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/hjel.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Sequence classification expressivity</h1> <p>Comparing expressivity to learnability</p> </d-title> <d-article> <p>This is a writeup of my miniproject for the <a href="https://www.cl.cam.ac.uk/teaching/2324/DNN/" rel="external nofollow noopener" target="_blank">DNN</a> module. The source code is available <a href="https://github.com/Hjel2/dnn-expressivity" rel="external nofollow noopener" target="_blank">on github</a>.</p> <blockquote> <p>It‚Äôs great work, and you clearly went beyond the brief. -Ferenc Husz√°r</p> </blockquote> <p>This project investigates the expressive power of sequence classification models by comparing them to formal grammars. Specifically, I lower-bounded the complexity of languages which different sequence classification architectures can theoretically recognise on the <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy" rel="external nofollow noopener" target="_blank">Chomsky Hierarchy</a>, and compared this to the languages which they empirically learnt to recognise.</p> <h3 id="findings">Findings</h3> <p>Through the experiments in this project, I was able to discover:</p> <ol> <li>Learnability is a subset of expressivity</li> <li>Models <em>can</em> perfectly represent many languages</li> <li>Models <em>do not</em> perfectly learn even simple languages</li> <li>Recurrence as an inductive bias helps models approximate finite automata</li> <li>Efficient transformers are less expressive than transformers</li> </ol> <hr> <h2 id="methodology">Methodology</h2> <p>The primary contribution of this project is to demonstrate a separation of the grammars representable by models, and those which they learn. To achieve this, I train sequence to sequence models on datasets belonging to different formal grammar classes. I hardcode weights or use <a href="https://github.com/srush/raspy" rel="external nofollow noopener" target="_blank">RASPy</a><d-cite key="weiss2021thinking"> to theoretically demonstrate which languages can be represented.</d-cite></p> <p>During this project, I investigated the expressivity of RNN, GRNN and transformer architectures. Latter experiments also tested some efficient transformer architectures: linformer<d-cite key="wang2020linformer"></d-cite> and sparse transformer<d-cite key="child2019generating">.</d-cite></p> <h3 id="designing-information-retrieval-datasets">Designing information retrieval datasets</h3> <p>The project must first empirically provide lower bounds on the expressivity of languages recognisable by sequence to sequence models. To this end, I constructed three synthetic information retrieval datasets. These datasets were dictionary-lookup tasks with keys in different grammar classes. Each dataset consisted of sequences of random symbols with a key immediately followed by a value.</p> <ul> <li>the regular dataset key was \(üîë\)</li> <li>the context-free dataset keys were of the form \(üóùÔ∏è^nüîë^n\)</li> <li>the context-sensitive dataset keys were of the form \(üóùÔ∏è^nüîë^nüóùÔ∏è^n\)</li> </ul> <p>I constructed these datasets by generating random sequences, inserting keys and then running a vectorised state machine to recognise and then remove any spurious keys. Models output whether the key had occurred, and if so what the value was. This ‚Äòessentially‚Äô turned sequence to sequence models into sequence classification models.</p> <h3 id="how-can-we-prove-how-expressive-models-are">How can we prove how expressive models are?</h3> <p>Let the dataset \(d_g\) be a key-value retrieval dataset with keys drawn from grammar class \(g\). If a model \(m\) trained on the dataset \(d_g\) is able to perform well, then the languages learnable by \(m\) is not a subset of the grammar class \(g\).</p> <p>Based on this observation, I train RNNs, GRNNs and transformers on the datasets I have constructed. Through this, I am able to lower-bound the complexity of grammar classes which are representable by different models.</p> <h3 id="rnns-struggle-to-recognise-some-regular-languages">RNNs struggle to recognise some regular languages</h3> <p>Recurrent neural networks (RNNs) are the oldest sequence to sequence models. They have an implicit bias towards recurrence: RNNs process data sequentially in order. This means RNNs mimic the operation of a finite automaton, and as such should theoretically be able to learn simpler grammars classes.</p> <p>Unfortunately, my experiments demonstrated that recurrent neural networks do not consistently learn even simple regular languages. As seen in</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rnn_reglang-480.webp 480w,/assets/img/rnn_reglang-800.webp 800w,/assets/img/rnn_reglang-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rnn_reglang.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Visualisation of the performance of an RNN when trained on the regular key-value retrieval dataset. </div> <p>Critically, I observed that the RNN had great difficultly with the regular language dataset. Demonstrated by it typically converging to undesirable solutions: often getting stuck at 60-70% accuracy. During testing, it did converge to 95%+ accuracy sometimes, which demonstrated that it can learn to approximate the computation of a DFA.</p> <p>Given that this is a comparatively simple sanity-check, it is surprising that the RNN is struggling. RNNs obsolescence in modern literature represents these limitations.</p> <p>To determine whether the inability of RNNs to learn the regular language was due to a fundamental inability to represent the grammar, I set out to hardcode the weights for an RNN which recognised the language. After many hours of maths, I was able to explicitly construct RNN weights which caused the model to perfectly mimic the behaviour of a DFA, achieving 100\% accuracy. To demonstrate that all regular languages are representable by a DFA, I then hardcoded weighs for the context free dataset. Through this, I proved that RNNs are able to represent regular languages.</p> <p>The above theoretical results imply that the inability of RNNs to learn the simple regular language is due to the limitations in trainability rather than expressivity.</p> <h3 id="grnns-recognise-many-languages">GRNNs recognise many languages</h3> <p>My analysis of RNNS demonstrated that their inability to perform was largely due to training limitations rather than expressivity problems. Gated RNNs (GRNNs) improve upon the original RNN architecture and make them more trainable. To provide theoretical justification to this, I investigated whether GRNNs and RNNs are separable by formal grammars.</p> <p>Empirically, I found that GRNNs were able to recognise regular and context free grammars to a high level. Furthermore, they took steps towards recognising context-sensitive grammars. However, they were unable to fully learn context-sensitive grammars.</p> <p>Human grammar requires functionally all features of context-free grammars ‚Äì and some context-sensitive features. Since I have demonstrated that GRNNs struggle to learn context-sensitive features, it is unlikely that GRNNs would be able to fully represent human language. This means that they are unlikely to be able to form the foundation for the most powerful models.</p> <h3 id="transformers">Transformers</h3> <p>Transformers emerged in 2019 and caused rapid improvements in SOTA across every major subdiscipline of ML. They are fundamentally different to the previous models and are based on the attention mechanism. Each token is able to directly attend to all previous tokens, bypassing the ‚Äòexponential forgetting problem‚Äô and removing the recursive inductive bias.</p> <p>I trained the transformer all three datasets: regular, contxet free and context sensitive. The transformer was able to learn the regular and context free datasets. Furthermore, it was able to approximate context-sensitive languages. But they struggled to generalise to longer sequence lengths.</p> <p>I validated that transformers are able to represent regular and context free grammars by using <a href="https://github.com/srush/raspy" rel="external nofollow noopener" target="_blank">RASPy</a><d-cite key="weiss2021thinking"> to implement programs which achieve perfect performance on the regular and context free datasets.</d-cite></p> <h3 id="do-models-recognise-context-sensitivity-or-approximate-them">Do models <em>recognise</em> context sensitivity or approximate them?</h3> <p>The lengths of the keys for the context sensitive dataset were drawn from an exponential with parameter \(\mu\). By adjusting the value of \(\mu\), I can adjust the distribution from which keys are drawn. This allows me to test whether the model is truly learning the grammar, or merely statistical relations. If the model has learnt the grammar, then adjusting the value of \(\mu\) will have negligible effect on model behaviour. If, however, the model is merely approximating the grammar, then adjusting \(\mu\) will move it into OOD and cause significant performance degradation.</p> <div class="col mt-2"> <div class="row-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grnn_contextsenslang-480.webp 480w,/assets/img/grnn_contextsenslang-800.webp 800w,/assets/img/grnn_contextsenslang-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/grnn_contextsenslang.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grnn_contextsenslang_mu-480.webp 480w,/assets/img/grnn_contextsenslang_mu-800.webp 800w,/assets/img/grnn_contextsenslang_mu-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/grnn_contextsenslang_mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Demonstration in the variation in performance for a GRNN as the distribution from which the key to the dataset is drawn. The model was trained with parameter $\mu=2$. Notice the significant performance degredation as $\mu$ varies. This demonstrates that GRNN is not learning the grammar, but only a weak approximation to it. </div> <div class="col mt-2"> <div class="row-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_contextsenslang-480.webp 480w,/assets/img/transformer_contextsenslang-800.webp 800w,/assets/img/transformer_contextsenslang-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/transformer_contextsenslang.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_contextsenslang_mu-480.webp 480w,/assets/img/transformer_contextsenslang_mu-800.webp 800w,/assets/img/transformer_contextsenslang_mu-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/transformer_contextsenslang_mu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Demonstration in the variation in performance for a Transformer as the distribution from which the key to the dataset is drawn. The model was trained with parameter $\mu=2$. The transformer did not have significant performance degradation as the distribution from which the keys were drawn varied. This supports the hypothesis that transformers are more expressive than GRNNs, and that they are able to represent natural language. </div> <p>From the above figures, we notice that transformers retained their performance when the distribution from which keys were drawn changed. This implies that they were able to better represent the context sensitive grammar.</p> <h3 id="efficient-transformers-are-fundamentally-less-expressive-than-transformers">Efficient transformers are fundamentally less expressive than transformers</h3> <p>The above experiments support the hypothesis that transformers are fundamentally more expressive than other sequence to sequence models. However, transformers are also more computationally expensive. This means that much research has investigated efficient transformers. It is summarised below, in a venn diagram constructed by Efficient Transformers: A Survey<d-cite key="tay2022efficient">.</d-cite></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Venn_Transformers-480.webp 480w,/assets/img/Venn_Transformers-800.webp 800w,/assets/img/Venn_Transformers-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/Venn_Transformers.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Venn diagram demonstrating the types of efficient transformer. </div> <p>This diagram shows that there are a wide range of transformers, whose methods of increasing efficiency can be split into a few categories. I continued my investigation by selecting and reimplementing two efficient transformer architectures. I chose Linformer and Sparse Transformer since they are different types of efficient transformer.</p> <h4 id="linformer-limits-expressivity">Linformer limits expressivity</h4> <p>Linformers are efficient transformers. They operate on constant-length datasets and have no causal mask. This means inference time is \(\mathcal{O}(n)\) for sequence length \(n\). As such, it is much more time efficient than the standard transformer architecture.</p> <p>I replicated a Linformer and tested it on the datasets. It was unable to learn the language and constantly overfit. After significant experimentation, and repeating my experiments with the official implementation, I concluded that Linformer struggled to recognise even regular languages. Instead, Linformer consistently overfit to the training data and refused to approximate a DFA, even when provided extensive amounts of training data. I was unable to construct a situation where the Linformer approximated a DFA and did not overfit.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/linformer_reglang-480.webp 480w,/assets/img/linformer_reglang-800.webp 800w,/assets/img/linformer_reglang-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/linformer_reglang.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Diagram demonstrating the performance and generalisation of Linformer on a the regular langauge dataset. </div> <p>Linformer does not have a causal mask. I hypothesised that the lack of a causal mask meant that Linformer struggled to approximate the behaviour of a finite automaton. To test this hypothesis, I removed the causal mask from a transformer and tested its performance. I found that it caused a significant performance drop, however not to the same extent as the Linformer. From this, I was able to conclude that there is a significant gap between the langauges learnable by a Linformer and by a Transformer.</p> <h4 id="sparse-transformers">Sparse Transformers</h4> <p>Sparse Transformers have time complexity of \(\mathcal{O}(n\cdot \lg n)\) for sequence length \(n\), which they achieve by limiting the flow of information.</p> <p>I repeated the experiments above with Sparse Transformers. I found that due to the limited information flow, they required much deeper networks to propagate information in the same way as earlier. Transformers were able to achieve near-perfect accuracy on the regular language dataset with 3 layers. Sparse Transformers had such throttled information flow such that they would require 4 layers to propagate information to the next position. I was therefore only able to elicit performance out of 8-layer Sparse Transformers. This explicitly demonstrates how much less expressive Sparse Transformers (of fixed size) are than Transformers. Notably, I did not find Sparse Transformers to be fundamentally less expressive: they may be better suited for other tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sparsetransformer_reglang-480.webp 480w,/assets/img/sparsetransformer_reglang-800.webp 800w,/assets/img/sparsetransformer_reglang-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/sparsetransformer_reglang.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Diagram demonstrating the performance and generalisation of a very deep Sparse Transformer on a the regular langauge dataset. </div> <h2 id="sequence-classification-models-learn-robust-approximations">Sequence classification models learn robust approximations</h2> <p>I concluded by investigating the robustness of the approximations learnt on the regular language dataset. The theory is that if models learnt the actual function or a robust approximation, then adversarial attacks would be difficult to generate. However, if they learnt poor approximations, then adversarial attacks would be easy!</p> <p>I found that black-box greedy attacks are easy. However, generating examples efficiently using white-box gradient-based methods proved ineffective. During my explorations, I implemented a greedy attack, Projected FGSM<d-cite key="papernot2016crafting">, Saliency FGSM<d-cite key="yang2020greedy">, an adaptation of Saliency FGSM using cosine similarity, and a greedy search for universal adversarial suffices.</d-cite></d-cite></p> <p>I found that attacks which exploited gradient-information were unlikely to succeed. I attribute this to the discrete nature of the optimisation problem. Greedy attacks were able to succeed on both GRNNs and Transformers for longer context lengths. This implies that the models were able to learn the simple dataset and learnt robust approximations.</p> <div class="col mt-2"> <div class="row-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/grnn_reglang_adv-480.webp 480w,/assets/img/grnn_reglang_adv-800.webp 800w,/assets/img/grnn_reglang_adv-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/grnn_reglang_adv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer_reglang_adv-480.webp 480w,/assets/img/transformer_reglang_adv-800.webp 800w,/assets/img/transformer_reglang_adv-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/transformer_reglang_adv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Visualisation of the performance of greedy attacks on the regular langauge dataset on GRNNs and transformers. </div> <h2 id="conclusion">Conclusion</h2> <p>Overall, this project served to make the separation of theoretical expressivity and the learnable expressivity explicit. Additionally, it demonstrated how inductive biases help models to approximate finite automaton, which is demonstrated by GRNNs performing similarly to transformers in this situation, despite being outclassed by them in larger tasks.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/expressivity.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> ¬© Copyright 2025 Harry Langford. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>